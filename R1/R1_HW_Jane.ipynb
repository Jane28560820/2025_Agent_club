{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOPXxjexDMDZ"
      },
      "source": [
        "# baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hvn98KdEE9_"
      },
      "source": [
        "將`關鍵字`比對換成`向量相似度`比對。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4-rfIDDz7D"
      },
      "source": [
        "> 請將目前使用關鍵字比對的 route_by_query，改為使用向量相似度進行分類，並設一個合理的相似度門檻，根據檢索結果的分數判斷是否走 RAG 流程。  \n",
        "例如用向量相似度及自訂 threshold 決定要不要分到 retriever。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2jMeAOuJ2OM"
      },
      "source": [
        "> Hint：similarity_search_with_score(...)  \n",
        "可參考去年的讀書會 R4：向量資料庫的基本操作"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langgraph transformers bitsandbytes langchain-huggingface langchain-community chromadb"
      ],
      "metadata": {
        "id": "AJMsnje8B6Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# RAG需要的文件\n",
        "docs_text = \"\"\"\n",
        "火影代數\t姓名\t師傅\t徒弟\n",
        "初代\t千手柱間\t無明確記載\t猿飛日斬、水戶門炎、轉寢小春\n",
        "二代\t千手扉間\t千手柱間（兄長）\t猿飛日斬、志村團藏、宇智波鏡等\n",
        "三代\t猿飛日斬\t千手柱間、千手扉間\t自來也、大蛇丸、千手綱手（傳說三忍）\n",
        "四代\t波風湊\t自來也\t旗木卡卡西、宇智波帶土、野原琳\n",
        "五代\t千手綱手\t猿飛日斬\t春野櫻、志乃等（主要為春野櫻）\n",
        "六代\t旗木卡卡西\t波風湊\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\n",
        "七代\t漩渦鳴人\t自來也、旗木卡卡西\t木葉丸等（主要為木葉丸）\n",
        "\"\"\"\n",
        "\n",
        "# RAG 系統中每一段文本都需要封裝成 Document\n",
        "docs = [Document(page_content=txt.strip()) for txt in docs_text.strip().split(\"\\n\\n\")]\n",
        "\n",
        "# chromadb 預設使用的大型語言模型為 \"all-MiniLM-L6-v2\"，由於該大型語言模型不支持中文，所以將模型替換為 \"infgrad/stella-base-zh-v3-1792d\"，並對 embedding 進行量化\n",
        "\n",
        "# 建立中文 embedding 模型\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"infgrad/stella-base-zh-v3-1792d\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# 建立 Chroma 向量資料庫並儲存\n",
        "persist_path = \"document_store\"\n",
        "collection_name = \"naruto_collection\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_path,\n",
        "    collection_name=collection_name\n",
        ")"
      ],
      "metadata": {
        "id": "I_lbCeJyBmz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# 1. 生成回應模型載入\n",
        "\n",
        "# 使用 4-bit 量化模型\n",
        "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
        "\n",
        "# 設定量化參數\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# 載入 tokenizer 與 4-bit 模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 建立 text generation pipeline\n",
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # 僅返回生成的回應內容\n",
        ")"
      ],
      "metadata": {
        "id": "-BxVG8dGBm8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 定義狀態（State）\n",
        "from typing_extensions import TypedDict, List\n",
        "\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class RAGState(TypedDict):\n",
        "    query: str            #使用者問題\n",
        "    docs: List[Document]  #檢索到的文件\n",
        "    answer: str   #模型答案\n",
        "\n",
        "\n",
        "# 3. 定義節點（Node）\n",
        "\n",
        "# 用來 retrive 歷代火影資料的節點\n",
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    # similarity_search 距離越小越相似\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": \"\"}\n",
        "\n",
        "# 用來 retrive 後生成的節點\n",
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    query, docs = state[\"query\"], state[\"docs\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
        "        f\"內容：{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": output}\n",
        "\n",
        "# 直接生成的節點\n",
        "def direct_generate_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    prompt = f\"請回答以下問題：{query}\\n\\n回答：\"\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": [], \"answer\": output}\n",
        "\n",
        "# 定義 Route Node 條件分支（決定走哪條路）\n",
        "# def route_by_query(state):\n",
        "#     query = state[\"query\"]\n",
        "#     threshold = 0.8  # 數值越小代表越相似\n",
        "\n",
        "#     # 使用語意檢索（距離越小越相似）\n",
        "#     results_with_score = vectorstore.similarity_search_with_score(query, k=1)\n",
        "\n",
        "#     if results_with_score:\n",
        "#         _, score = results_with_score[0]  # 取出最相似的一筆資料的分數\n",
        "#         print(f\"相似度分數：{score:.4f}\")\n",
        "#         if score < threshold:\n",
        "#             choice = \"naruto\"\n",
        "#             print(f\"跑到 → {choice}\")\n",
        "#             return choice\n",
        "#         else:\n",
        "#             choice = \"general\"\n",
        "#             print(f\"跑到 → {choice}\")\n",
        "#             return choice\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def route_by_query(state):\n",
        "    query = state[\"query\"]\n",
        "    query_embedding = embedding_model.embed_query(query)  # 取得 query 的向量\n",
        "\n",
        "    # 抓出向量庫中最接近的 doc 和其向量\n",
        "    results_with_score = vectorstore.similarity_search_with_score(query, k=1)\n",
        "\n",
        "    if results_with_score:\n",
        "        doc, _ = results_with_score[0]\n",
        "        doc_embedding = embedding_model.embed_query(doc.page_content)  # 取得文件的向量\n",
        "\n",
        "        # 計算 cosine similarity\n",
        "        sim = cosine_similarity(\n",
        "            np.array(query_embedding).reshape(1, -1),\n",
        "            np.array(doc_embedding).reshape(1, -1)\n",
        "        )[0][0]\n",
        "\n",
        "        print(f\"Cosine similarity：{sim:.4f}\")\n",
        "\n",
        "        if sim > 0.7:  # 相似度越高越相似，這裡是你可以調的門檻\n",
        "            choice = \"naruto\"\n",
        "            print(f\"跑到 → {choice}\")\n",
        "            return choice\n",
        "\n",
        "        else:\n",
        "            choice = \"general\"\n",
        "            print(f\"跑到 → {choice}\")\n",
        "            return choice"
      ],
      "metadata": {
        "id": "d4bwytelBnBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDN-jWJhNNNr"
      },
      "outputs": [],
      "source": [
        "# 4. 建立 LangGraph 流程圖（StateGraph）\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 建立 LangGraph 流程圖\n",
        "graph_builder = StateGraph(RAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"condition\")\n",
        "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 設定條件分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"condition\",\n",
        "    path=RunnableLambda(route_by_query),\n",
        "    path_map={\n",
        "        \"naruto\": \"retriever\",\n",
        "        \"general\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 接下來的正常連接\n",
        "graph_builder.add_edge(\"retriever\", \"generator\")\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wflAEri34xe5",
        "outputId": "49b19424-9952-4eda-a8c6-e82d46b60853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 誰是第四代火影?\n",
            "Cosine similarity：0.7023\n",
            "跑到 → naruto\n",
            "回答： 第四代火影是波風湊。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的師傅是誰?\n",
            "Cosine similarity：0.7398\n",
            "跑到 → naruto\n",
            "回答： 第四代火影的師傅是自來也。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的徒弟有哪些人?\n",
            "Cosine similarity：0.7476\n",
            "跑到 → naruto\n",
            "回答： 第四代火影的徒弟有旗木卡卡西、宇智波帶土、野原琳。\n",
            "============================================================ \n",
            "\n",
            "使用者: 相對論是誰發明的?\n",
            "Cosine similarity：0.3545\n",
            "跑到 → general\n",
            "回答： 相對論是由愛因斯坦（Albert Einstein）在 1905 年所發明的。\n",
            "============================================================ \n",
            "\n",
            "使用者: quit\n",
            "掰啦！\n"
          ]
        }
      ],
      "source": [
        "# 5. 建立 RAG 結果\n",
        "\n",
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    # 初始化 State（RAGState）\n",
        "    init_state: RAGState = {\n",
        "        \"query\": user_input,\n",
        "        \"docs\": [],\n",
        "        \"answer\": \"\"\n",
        "    }\n",
        "\n",
        "    result = graph.invoke(init_state) # 執行 LangGraph 流程圖\n",
        "    raw_output = result[\"answer\"]\n",
        "\n",
        "    answer_text = raw_output.split(\"回答：\")[-1].strip()\n",
        "    print(\"回答：\", answer_text)\n",
        "    print(\"===\" * 20, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8blDsDnDpbO"
      },
      "source": [
        "# advance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofkLZjzHENNT"
      },
      "source": [
        "改成能支援多輪問答（Multi-turn RAG），並能根據前面的query判斷問題。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2VXz7FxEONI"
      },
      "source": [
        "> 請將 RAGState 加入 history 欄位，並在生成回答時，將歷史對話與當前問題一起組成 prompt。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eSvoKFiKqP5"
      },
      "source": [
        "> Hint：\n",
        "```\n",
        "class MultiTurnRAGState(TypedDict):  \n",
        "    history: List[str]  \n",
        "    query: str  \n",
        "    docs: List[Document]  \n",
        "    answer: str\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 定義狀態（State）\n",
        "from typing_extensions import TypedDict, List\n",
        "\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class MultiTurnRAGState(TypedDict):\n",
        "    history: List[str]\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str\n",
        "\n",
        "# 3. 定義節點（Node）\n",
        "\n",
        "# 用來 retrive 歷代火影資料的節點\n",
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    # similarity_search 距離越小越相似\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": \"\"}\n",
        "\n",
        "# 加入history後 用來 retrive 後生成的節點\n",
        "def generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query, docs, history = state[\"query\"], state[\"docs\"], state[\"history\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    # 將歷史對話串成一段對話文本\n",
        "    history_text = \"\\n\".join([f\"對話{i+1}：{msg}\" for i, msg in enumerate(history)])\n",
        "\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據歷史對話與下列內容回答使用者的問題。\\n\\n\"\n",
        "        f\"{history_text}\\n\\n\"\n",
        "        f\"文件內容：\\n{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "\n",
        "    # 更新歷史對話（加入這輪問題與回答）\n",
        "    new_history = history + [f\"使用者：{query}\", f\"AI：{output}\"]\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": output, \"history\": new_history}\n",
        "\n",
        "\n",
        "\n",
        "# 加入history後的 直接生成的節點\n",
        "def direct_generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query, history = state[\"query\"], state[\"history\"]\n",
        "    history_text = \"\\n\".join([f\"對話{i+1}：{msg}\" for i, msg in enumerate(history)])\n",
        "\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據歷史對話回答使用者的問題。\\n\\n\"\n",
        "        f\"{history_text}\\n\\n\"\n",
        "        f\"請回答以下問題：{query}\\n\\n回答\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "\n",
        "    new_history = history + [f\"使用者：{query}\", f\"AI：{output}\"]\n",
        "    return {\"query\": query, \"docs\": [], \"answer\": output, \"history\": new_history}\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def route_by_query(state):\n",
        "    query = state[\"query\"]\n",
        "    history = state.get(\"history\", [])\n",
        "\n",
        "    # 組合語意查詢內容（以最近兩句為上下文）\n",
        "    if history:\n",
        "        recent_history = \"\\n\".join(history[-2:])  # 最後一輪對話（使用者 + AI）\n",
        "        full_query = f\"{recent_history}\\n{query}\"\n",
        "    else:\n",
        "        full_query = query\n",
        "\n",
        "    query_embedding = embedding_model.embed_query(full_query)\n",
        "\n",
        "    results_with_score = vectorstore.similarity_search_with_score(query, k=1)\n",
        "    if results_with_score:\n",
        "        doc, _ = results_with_score[0]\n",
        "        doc_embedding = embedding_model.embed_query(doc.page_content)\n",
        "\n",
        "        sim = cosine_similarity(\n",
        "            np.array(query_embedding).reshape(1, -1),\n",
        "            np.array(doc_embedding).reshape(1, -1)\n",
        "        )[0][0]\n",
        "\n",
        "        print(f\"Cosine similarity：{sim:.4f}\")\n",
        "        if sim > 0.6:\n",
        "            choice = \"naruto\"\n",
        "            print(f\"跑到 → {choice}\")\n",
        "            return choice\n",
        "        else:\n",
        "            choice = \"general\"\n",
        "            print(f\"跑到 → {choice}\")\n",
        "            return choice"
      ],
      "metadata": {
        "id": "zz28RHOlPqv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4QWyJxhP5Ir"
      },
      "outputs": [],
      "source": [
        "# 4. 建立 LangGraph 流程圖（StateGraph）\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 改成 MultiTurnRAGState 建立 LangGraph 流程圖\n",
        "graph_builder = StateGraph(MultiTurnRAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"condition\")\n",
        "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 設定條件分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"condition\",\n",
        "    path=RunnableLambda(route_by_query),\n",
        "    path_map={\n",
        "        \"naruto\": \"retriever\",\n",
        "        \"general\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 接下來的正常連接\n",
        "graph_builder.add_edge(\"retriever\", \"generator\")\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Jrna_evTCC",
        "outputId": "f5d0fc49-d0df-4a10-a4ca-52a9e4c09b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 第四代火影是誰?\n",
            "Cosine similarity：0.6912\n",
            "跑到 → naruto\n",
            "history: []\n",
            "AI 助理: 第四代火影是波風湊。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 他的師父是誰?\n",
            "Cosine similarity：0.7134\n",
            "跑到 → naruto\n",
            "history: ['使用者：第四代火影是誰?', 'AI：第四代火影是波風湊。']\n",
            "AI 助理: 他的師父是自來也。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 他的徒弟有哪些人?\n",
            "Cosine similarity：0.6509\n",
            "跑到 → naruto\n",
            "history: ['使用者：第四代火影是誰?', 'AI：第四代火影是波風湊。', '使用者：他的師父是誰?', 'AI：他的師父是自來也。']\n",
            "AI 助理: 他的徒弟有旗木卡卡西、宇智波佐助、春野櫻（第七班）。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 相對論是他發明的嗎?\n",
            "Cosine similarity：0.6484\n",
            "跑到 → naruto\n",
            "history: ['使用者：第四代火影是誰?', 'AI：第四代火影是波風湊。', '使用者：他的師父是誰?', 'AI：他的師父是自來也。', '使用者：他的徒弟有哪些人?', 'AI：他的徒弟有旗木卡卡西、宇智波佐助、春野櫻（第七班）。']\n",
            "AI 助理: 相對論並非是第四代火影波風湊所發明的。相對論是一種理論，主要由愛因斯坦在1905年提出，它描述了重力和加速度等現象。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: quit\n",
            "掰啦！\n"
          ]
        }
      ],
      "source": [
        "global_history: List[str] = []\n",
        "\n",
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    state = {\n",
        "    \"query\": user_input,\n",
        "    \"docs\": [],\n",
        "    \"answer\": \"\",\n",
        "    \"history\": global_history\n",
        "    }\n",
        "\n",
        "    result = graph.invoke(state)\n",
        "\n",
        "    answer = result[\"answer\"].split(\"回答：\")[-1].strip()\n",
        "    print(\"history:\",global_history)\n",
        "    print(\"AI 助理:\", answer)\n",
        "    print(\"===\" * 60, \"\\n\")\n",
        "\n",
        "    global_history = result[\"history\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYHUPcYTNW6p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}